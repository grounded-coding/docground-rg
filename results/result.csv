team_id,entry_id,detection_prec,detection_rec,detection_f1,selection_prec,selection_rec,selection_f1,selection_em_acc,generation_bleu,generation_meteor,generation_rouge_1,generation_rouge_2,generation_rouge_l,overall_mrr,pretrained,external_api,ensemble,desc
0,0,0.9982123703968536,0.997855611150822,0.998033958891868,0.7901287187793027,0.7877113962777258,0.7889182058047494,0.3905747947161728,0.1004303349412369,0.1748155417189704,0.3519862318674445,0.1429562911520304,0.2752605706967929,0.039439072637602,BART-base,No,"No, only a single model was used for the response generation",Baseline
1,0,0.9976851851851852,0.770192994996426,0.8693021379588544,0.7871916823360248,0.6048270587235489,0.6840638216070742,0.3089546914020692,0.087280557858524,0.1511779349704869,0.3060813538054137,0.1248065242676638,0.2400413409160427,0.0226151455594977,bart-base,no,"No, only a single model was used for the response generation",We used the prompt method to try to improve on top of baseline.
2,0,0.9939523301316258,0.9985704074338814,0.9962560171153504,0.8093487394957983,0.7857567774284014,0.7973782933034367,0.4156305506216696,0.1050077518059074,0.1690627113079486,0.3472022734137167,0.1466027591857049,0.2792812764058823,0.065063069407286,google/long-t5-tglobal-base,NIL,"No, only a single model was used for the response generation",Training method is identical to the provided baseline
2,1,0.9939523301316258,0.9985704074338814,0.9962560171153504,0.8093487394957983,0.7857567774284014,0.7973782933034367,0.4156305506216696,0.0944130837525522,0.1747103893740714,0.3651859460344296,0.1494439405687795,0.2874646392755053,0.1420833826645537,LLaMA 7B LoRA,NIL,"No, only a single model was used for the response generation","[2nd highest priority for human eval]. Finetuned LLaMA 7B using PEFT and LoRA on Stanford instruction data, then finetuned on track 5 data with prompting."
2,2,0.9939523301316258,0.9985704074338814,0.9962560171153504,0.8093487394957983,0.7857567774284014,0.7973782933034367,0.4156305506216696,0.1024044623736035,0.1776223726280943,0.3633414981394536,0.1508164461026875,0.286493668723885,0.088874091955263,facebook/bart-base; google/long-t5-tglobal-base; LLaMA 7B LoRA,Open AI GPT-4,"Yes, multiple model outputs were combined for the response generation","BART and Long-T5 were trained using the baseline method, LLaMA method described in entry #2. Dialogue history, knowledge snippets, and 3 outputs were rated by GPT-4 on a 1-5 scale and the highest scoring output for each example was picked."
2,3,0.9939523301316258,0.9985704074338814,0.9962560171153504,0.8093487394957983,0.7857567774284014,0.7973782933034367,0.4156305506216696,0.0984120454517721,0.1773642487883115,0.365757042048007,0.1508646448593742,0.2875421912716351,0.2348390644827355,facebook/bart-base; google/long-t5-tglobal-base; LLaMA 7B LoRA,Open AI GPT-4,"Yes, multiple model outputs were combined for the response generation","[Highest priority for human eval] BART and Long-T5 were trained using the baseline method, LLaMA method described in entry #2. Dialogue history, knowledge snippets, and 3 outputs were rated by GPT-4 on a 1-5 scale. We used the LLaMA output if it scored > 3, otherwise the highest scoring output was selected."
3,0,0.9985683607730852,0.9971408148677628,0.997854077253219,0.7566513224623547,0.8241692869890371,0.7889684347543119,0.3966440556943948,0.0830194415185609,0.1600382788504971,0.3333215593847494,0.1285948089347588,0.2638261740176144,0.0270217373554985,Flan-t5 small,not used,"No, only a single model was used for the response generation",multitask model was used
3,1,0.9985683607730852,0.9971408148677628,0.997854077253219,0.7566513224623547,0.8241692869890371,0.7889684347543119,0.3966440556943948,0.0873524492078511,0.1640656292335456,0.3424734413660397,0.137203772617767,0.2701914150284781,0.0295497265892002,flan-t5 base,not used,"No, only a single model was used for the response generation",multitask model was used
4,0,0.9982136477313326,0.9985704074338814,0.9983919957119884,0.8320014051110916,0.8051329990651823,0.8183467219486914,0.4612638343448768,0.0956579984338782,0.1661831271467274,0.3401581443917736,0.1311788906559898,0.2630822593993342,0.0389921904434288,bart-base,no,"No, only a single model was used for the response generation",T5 query generator
4,1,0.9982136477313326,0.9985704074338814,0.9983919957119884,0.8182866556836903,0.8442253760516699,0.831053666290208,0.4894680471260264,0.1003452759350566,0.1730897239221265,0.3510434563966231,0.142201272506334,0.2737314373325399,0.051432313989029,T5  & bart-base,no,"No, only a single model was used for the response generation",T5 query generator
4,2,0.9982136477313326,0.9985704074338814,0.9983919957119884,0.8320014051110916,0.8051329990651823,0.8183467219486914,0.4612638343448768,0.0636077298479178,0.1592049194173961,0.3505053310063095,0.1429802253444485,0.2748457566328739,0.0404510238277068,T5 & Bart-base,o,"No, only a single model was used for the response generation",no
4,3,0.9982136477313326,0.9985704074338814,0.9983919957119884,0.7413501460864217,0.8194102150080734,0.7784281273967627,0.4041413780792574,0.0629463228184403,0.1596001132111345,0.3496189716122523,0.1423041980629968,0.2734700584999153,0.0338735863897625,T5 & Bart base,no,"No, only a single model was used for the response generation",T5 query generator
4,4,0.9982136477313326,0.9985704074338814,0.9983919957119884,0.783147753297007,0.8427806577717345,0.8118706508391322,0.493576017130621,0.0638836154463558,0.1612349685637306,0.3525603535947395,0.14409235299075,0.2749374646264599,0.0421060043757412,T5 & Bart-base,no,"No, only a single model was used for the response generation",T5 query generator
5,0,0.9985678481919084,0.996783416726233,0.9976748345555356,0.7740621156211562,0.8556981388629217,0.8128355196770939,0.4675,0.0962672317248282,0.171890549167278,0.3470368363202828,0.1379327181511477,0.2691633649279964,0.0378173884288014,facebook/bart-base,x,"No, only a single model was used for the response generation",same as baseline
5,1,0.9985678481919084,0.996783416726233,0.9976748345555356,0.7861488457371447,0.801648678507691,0.7938231086425985,0.4096428571428571,0.0985653547235657,0.171313471093617,0.3482846367551636,0.1401198451450541,0.2714096435212944,0.0350336374341024,facebook/bart-base,x,"No, only a single model was used for the response generation",same as baseline
5,2,0.9985678481919084,0.996783416726233,0.9976748345555356,0.6494832124285806,0.8597773434180335,0.739979520187244,0.4214846538187009,0.0938617311028783,0.1669382175436921,0.3391073194801601,0.1335345185171307,0.264604804586969,0.03005193325917,facebook/bart-base,x,"No, only a single model was used for the response generation",same as baseline
6,0,0.9967925873129008,0.9996426018584704,0.9982155603140612,0.8039398894339329,0.8774538964901845,0.8390898008939457,0.5547237076648841,0.1017355265959685,0.1893561817805288,0.3629452611888367,0.1477720298604739,0.2803997714648423,0.212702922077922,bart-large,No,"Yes, multiple model outputs were combined for the response generation",We fine-tune bart-large model on the training set. The final submission is obtained by an ensemble of the result of different model outputs using different random seeds.
6,1,0.9967925873129008,0.9996426018584704,0.9982155603140612,0.7607449856733525,0.9025240078184754,0.8255917907256968,0.5384889522451889,0.100785994526871,0.1889427292346996,0.3616120716065222,0.1466832314665592,0.2781649848691553,0.1251411782661782,bart-large,No,"Yes, multiple model outputs were combined for the response generation",We fine-tune bart-large model on the training set. The final submission is obtained by an ensemble of the result of different model outputs using different random seeds.
6,2,0.9967925873129008,0.9996426018584704,0.9982155603140612,0.8124901559300677,0.8767740290643324,0.843408951563458,0.5691375623663578,0.1004555442726301,0.1886145492117905,0.3617375527754671,0.1464153834295622,0.2793664883394965,0.1239400584795321,bart-large,No,"Yes, multiple model outputs were combined for the response generation",The output of the knowledge selection model of every single model is input to each dialogue generation model respectively. The dialogue generation model generates different outputs according to different knowledge inputs. The final result is obtained by a model ensemble of all these generated outputs.
6,3,0.9967925873129008,0.9996426018584704,0.9982155603140612,0.7711771977023195,0.9013342398232344,0.8311912225705329,0.5538132573057734,0.1004555442726301,0.1886145492117905,0.3617375527754671,0.1464153834295622,0.2793664883394965,0.1103983918128655,bart-large,No,"Yes, multiple model outputs were combined for the response generation",The output of the knowledge selection model of every single model is input to each dialogue generation model respectively. The dialogue generation model generates different outputs according to different knowledge inputs. The final result is obtained by a model ensemble of all these generated outputs.
7,0,0.9978579078900393,0.998927805575411,0.998392570101804,0.8096172718351324,0.8413359394917991,0.825171910814753,0.5210413694721826,0.0958589254441149,0.1804687082291274,0.3552031115065418,0.1450377759190828,0.2783998603247692,0.0647812785041045,BART-large,Did not use,"No, only a single model was used for the response generation","Following the baseline setting, using BART-large"
7,1,0.9978579078900393,0.998927805575411,0.998392570101804,0.8096172718351324,0.8413359394917991,0.825171910814753,0.5210413694721826,0.1047228538790857,0.1763805473362839,0.3603296755491524,0.1463217083767148,0.2800947486355599,0.0833009004884005,BART-base,Did not use,"No, only a single model was used for the response generation","Following the baseline setting, using BART-base"
7,2,0.9978579078900393,0.998927805575411,0.998392570101804,0.8096172718351324,0.8413359394917991,0.825171910814753,0.5210413694721826,0.0896529820755832,0.1743558553116951,0.3590918283999151,0.1458394659109244,0.2839761389272286,0.0662450256200256,T5-large,Did not use,"No, only a single model was used for the response generation","Following the baseline setting, using T5-large"
7,3,0.9978586723768736,0.9992852037169406,0.9985714285714284,0.8183304717521053,0.850599133169032,0.8341528460705058,0.5313837375178316,0.1050021666416961,0.1774318472553917,0.3617360399636435,0.1473515405566828,0.2805471106093161,0.2160141941391941,BART-base pre-trained with in-domain data,Did not use,"No, only a single model was used for the response generation","Following the baseline setting, using BART-base"
7,4,0.9978586723768736,0.9992852037169406,0.9985714285714284,0.8183304717521053,0.850599133169032,0.8341528460705058,0.5313837375178316,0.1074781447529546,0.1743771133742458,0.3585123879688949,0.1459177208825036,0.2793915761043494,0.240206608005521,T5-base,Did not use,"No, only a single model was used for the response generation","Following the baseline setting, using T5-base"
8,0,0.9978563772775992,0.9982130092923516,0.9980346614257638,0.8240151384827111,0.8141412424577208,0.8190484332920104,0.5130217623974314,0.1029111855369951,0.176381720642245,0.358711628955722,0.1479337927363981,0.2822433083620428,0.073638327938831,bart-large,None,"No, only a single model was used for the response generation","Data augmentation(Contextual word embedding, Back translation)"
8,1,0.9978563772775992,0.9982130092923516,0.9980346614257638,0.8240151384827111,0.8141412424577208,0.8190484332920104,0.5130217623974314,0.0444784063027588,0.1138914412966549,0.2800700930819199,0.1046233384543271,0.2275948024975604,0.0342687074829931,bart-large,None,"No, only a single model was used for the response generation","Data augmentation(Contextual word embedding, Back translation), Contrastive Search for neural text generation"
8,2,0.9967880085653104,0.9982130092923516,0.9975,0.6650415074449861,0.8578227245687091,0.7492299127853034,0.4041339985744832,0.0945689533945307,0.1667830233201119,0.340816628014796,0.1365831403619155,0.2666521302421155,0.0290905240195301,bart-base,None,"No, only a single model was used for the response generation","Data augmentation(Contextual word embedding, Back translation, ontology augmented dialog and knowledge)"
8,3,0.997855611150822,0.997855611150822,0.997855611150822,0.8238533689011273,0.8136313418883317,0.818710449803318,0.512665001783803,0.0951120609513152,0.1736943761716635,0.3518070940847518,0.1378605085140398,0.2705269586189079,0.0408931220604014,bart-large,None,"No, only a single model was used for the response generation","Data augmentation(Contextual word embedding, Back translation, ontology augmented dialog and knowledge)"
9,0,0.9995355318160706,0.7691208005718371,0.8693193294283983,0.8384940966807752,0.6397552477266933,0.725765244637262,0.3872811718470882,0.0730680019494204,0.147840728906359,0.3045677722827192,0.1151761962212148,0.232034811599595,0.0224180437327467,"chavinlo/alpaca-native, kevinscaria/atsc_tk-instruct-base-def-pos-neg-neut-combined, keybert",No,"No, only a single model was used for the response generation","Knowledge clustering with aspect-based sentiment classification by keybert and instructABSA, generate by alpaca with clustered knowledge promting."
9,1,0.9995355318160706,0.7691208005718371,0.8693193294283983,0.8384940966807752,0.6397552477266933,0.725765244637262,0.3872811718470882,0.0787913206173197,0.153152708745197,0.3140694498347285,0.1232830403399868,0.2405569850582903,0.0231547791197844,"allenai/PRIMERA, chavinlo/alpaca-native",No,"No, only a single model was used for the response generation","Conditional generation with PRIMERA on dialogue and knowledge, add normal question by alpaca."
9,2,0.9995355318160706,0.7691208005718371,0.8693193294283983,0.8384940966807752,0.6397552477266933,0.725765244637262,0.3872811718470882,0.0856292941108199,0.1472922393869973,0.3050801133322717,0.1203347196473519,0.2370891215713405,0.0228120871345161,"chavinlo/alpaca-native, facebook/bart-base, allenai/PRIMERA,kevinscaria/atsc_tk-instruct-base-def-pos-neg-neut-combined, keybert",No,"Yes, multiple model outputs were combined for the response generation","Cluster with aspect-based sentiment classification with keybert and instructABSA, generate by ensemble of alpaca, FiD+bart, primera."
10,0,0.9950142450142452,0.9985704074338814,0.9967891544773456,0.7955358664167661,0.793575252825699,0.7945543501382684,0.4177350427350427,0.1035382284809896,0.1791472281597527,0.3597962453732549,0.1473131873671629,0.2811653569623499,0.0674965220762634,flan-t5-large,no,"No, only a single model was used for the response generation",we changed the model and did hyperparameter tuning
10,1,0.9950142450142452,0.9985704074338814,0.9967891544773456,0.7955358664167661,0.793575252825699,0.7945543501382684,0.4177350427350427,0.0940989548654431,0.181432377362801,0.3556680976591373,0.1457686276351583,0.2788587999734013,0.0453956514940639,flan-t5-large,no,"No, only a single model was used for the response generation","We do post-processing on the output, adding a the most frequent question from the training set as follow-up question to each review summary. "
10,2,0.9957219251336898,0.9982130092923516,0.9969659111190432,0.7714560461575447,0.818135463584601,0.7941103687206137,0.423323823109843,0.0603234363129825,0.1210950934350333,0.2409381250006832,0.0873089673458205,0.1859004660760088,0.0250987808274674,-,Open AI's chatGPT API,"No, only a single model was used for the response generation","We prompted chatGPT with step-by-step instructions, to first generate a summary of the knowledge and then append a follow up question given the dialogue history. We provided three examples, selected from the most challenging cases in the training data according to the data trends on the validation set. Unfortunately, due to server problems, the data samples after 4500, are lacking a review summary."
11,0,0.9989270386266096,0.9982130092923516,0.998569896317483,0.6539532007009586,0.5391348687006033,0.5910191913545741,0.3071428571428571,0.0980302751440837,0.1614437038616693,0.3367631796882673,0.1385886097998244,0.2712967392461491,0.0466294641512171,google/longt5-tglobal-base,not used,"No, only a single model was used for the response generation","The main idea of our model is to utilize knowledge as much as possible when generating a response, in order to ensure that no relevant information is missed. To achieve this, our model uses the LongT5 to incorporate a larger amount of knowledge. The model's encoder uses the knowledge encoder trained in task2. 

Specifically, each knowledge is truncated using a length ratio to ensure that all knowledge is included in the model's input. The model also truncates the dialogue history from the back, as the latter sentence is generally more important for generating a response than the former."
11,1,0.9989270386266096,0.9982130092923516,0.998569896317483,0.6539532007009586,0.5391348687006033,0.5910191913545741,0.3071428571428571,0.0933674798058563,0.1617990727699252,0.3332572977855345,0.1295969398612386,0.2621928953695368,0.042549130239679,google/longt5-tglobal-base,not used,"No, only a single model was used for the response generation","The main idea of our model is to utilize knowledge as much as possible when generating a response, in order to ensure that no relevant information is missed. To achieve this, our model uses the LongT5 to incorporate a larger amount of knowledge. The model's encoder uses the knowledge encoder trained in task2. 

Specifically, each knowledge is truncated using a length ratio to ensure that all knowledge is included in the model's input. The model also truncates the dialogue history from the back, as the latter sentence is generally more important for generating a response than the former."
11,2,0.9989270386266096,0.9982130092923516,0.998569896317483,0.606328683225587,0.6057618764340954,0.60604514730264,0.1903571428571428,0.0982797878872537,0.1597670693024675,0.3318216019140055,0.1375465132797487,0.2692851110857502,0.0454851843971947,google/longt5-tglobal-base,not used,"No, only a single model was used for the response generation","The main idea of our model is to utilize knowledge as much as possible when generating a response, in order to ensure that no relevant information is missed. To achieve this, our model uses the LongT5 to incorporate a larger amount of knowledge. The model's encoder uses the knowledge encoder trained in task2. 

Specifically, each knowledge is truncated using a length ratio to ensure that all knowledge is included in the model's input. The model also truncates the dialogue history from the back, as the latter sentence is generally more important for generating a response than the former."
11,3,0.9989270386266096,0.9982130092923516,0.998569896317483,0.606328683225587,0.6057618764340954,0.60604514730264,0.1903571428571428,0.0962403200240929,0.1607294712129307,0.3309036497676748,0.1318174064060838,0.2629204941941977,0.0437292382799284,google/longt5-tglobal-base,not used,"No, only a single model was used for the response generation","The main idea of our model is to utilize knowledge as much as possible when generating a response, in order to ensure that no relevant information is missed. To achieve this, our model uses the LongT5 to incorporate a larger amount of knowledge. The model's encoder uses the knowledge encoder trained in task2.

Specifically, each knowledge is truncated using a length ratio to ensure that all knowledge is included in the model's input. The model also truncates the dialogue history from the back, as the latter sentence is generally more important for generating a response than the former."
12,0,0.9985704074338814,0.9985704074338814,0.9985704074338814,0.5477799469389798,0.9299736551372484,0.689453125,0.2819414703783012,0.0905877240465451,0.169224259523388,0.3386174987026521,0.1343433532391598,0.263884068257854,0.064852269555517,facebook/bart-base,no,"No, only a single model was used for the response generation",facebook/bart-base 
12,1,0.9985704074338814,0.9985704074338814,0.9985704074338814,0.7537958420929689,0.8227245687091017,0.7867533522958148,0.4291324526954659,0.0625921504595265,0.177371232520447,0.3365629403903498,0.1179697753421594,0.2443783063263246,0.070396075552665,no,Open AI's GPT API,"No, only a single model was used for the response generation",Open AI's GPT API
12,2,0.9985704074338814,0.9985704074338814,0.9985704074338814,0.7537958420929689,0.8227245687091017,0.7867533522958148,0.4291324526954659,0.0961274594601546,0.1715318834361022,0.3571599669694349,0.1466526779377521,0.2797745368947856,0.0811557793407218,flan_t5,no,"No, only a single model was used for the response generation",flan_t5
13,0,0.9964323938637174,0.9982130092923516,0.9973219068023566,0.8340923877683799,0.8715900399422113,0.8524290404355234,0.6566951566951567,0.1024262774726794,0.1826254019679992,0.3637516628642476,0.1524413012610557,0.2867746334709574,0.3000058356676003,t5-large,InstructGPT,"No, only a single model was used for the response generation",Data augmentation to generate mixed reviews / explicitly included a label in the input that indicates whether the reviews are mixed or not
13,1,0.9964323938637174,0.9982130092923516,0.9973219068023566,0.851062036412677,0.8580776748534036,0.8545554568152003,0.6474358974358975,0.1017031378565685,0.1829888606275841,0.3630120561328275,0.1530347322942316,0.2870385605390766,0.326982331394096,t5-large,InstructGPT,"No, only a single model was used for the response generation",Data augmentation to generate mixed reviews / explicitly included a label in the input that indicates whether the reviews are mixed or not
13,2,0.9964323938637174,0.9982130092923516,0.9973219068023566,0.8589942975635044,0.8449052434775218,0.8518915213572683,0.6431623931623932,0.1017202109325741,0.1819240945592739,0.3618276963895427,0.1514199721445561,0.2864542118965316,0.1640931372549019,t5-large,InstructGPT,"No, only a single model was used for the response generation",Data augmentation to generate mixed reviews / explicitly included a label in the input that indicates whether the reviews are mixed or not
13,3,0.9964323938637174,0.9982130092923516,0.9973219068023566,0.8589942975635044,0.8449052434775218,0.8518915213572683,0.6431623931623932,0.1081473857469432,0.1819428279441293,0.3651847045062412,0.1527635708688172,0.2872477048023455,0.3422181372549019,t5-large,None,"No, only a single model was used for the response generation",t5-large fine-tuned with dstc11 dataset
13,4,0.9964323938637174,0.9982130092923516,0.9973219068023566,0.8589942975635044,0.8449052434775218,0.8518915213572683,0.6431623931623932,0.093087419999746,0.1839795504547088,0.3590947170986578,0.1483826249969241,0.280779872098953,0.1450495582848524,t5-large,InstructGPT,"No, only a single model was used for the response generation","The T5 model generates individual responses for each review, which are then combined by instructGPT."
14,0,0.9978579078900393,0.998927805575411,0.998392570101804,0.785559986706547,0.8035183139287839,0.7944376759232029,0.4182726623840114,0.1066354675689707,0.1748178225267118,0.359936104109385,0.1576770355870017,0.2899250739994946,0.3276899744572158,T5,n/a.,"No, only a single model was used for the response generation",T5-v1.1-xl
14,1,0.9978579078900393,0.998927805575411,0.998392570101804,0.785559986706547,0.8035183139287839,0.7944376759232029,0.4182726623840114,0.0931190847374617,0.1755823469679102,0.3649762863529995,0.1494042345250062,0.2867307197403421,0.0946914241599039,t5,n/a.,"No, only a single model was used for the response generation",T5-v1.1-large
