{
"gradient_accumulation_steps": 8,
"train_micro_batch_size_per_gpu": 1,
"optimizer": {
    "type": "AdamW",
    "params": {
    "lr": 3e-5,
    "eps": 1e-8
    }
},
"fp16": {
    "enabled": true
},
"zero_optimization": {
    "stage": 3
},
"activation_checkpointing": {
    "partition_activations": true,
    "cpu_checkpointing": false,
    "contiguous_memory_optimization": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
    },
"scheduler": {
    "type": "WarmupLR",
    "params": {
        "warmup_num_steps": 1000,
        "warmup_type": "linear"
    }
}
}